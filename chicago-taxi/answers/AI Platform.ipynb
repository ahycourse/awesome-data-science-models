{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Define environment variables</b>\n",
    "\n",
    "To be used in future training steps.  Note that the BUCKET_NAME defined below must exist in the GCP project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: BUCKET_NAME=ml-workshop-chicago-taxi-demo\n",
      "env: LOCAL_JOB_DIR=local-training-output\n",
      "env: JOB_NAME=keras_job_20200615_085659\n",
      "env: REGION=us-central1\n",
      "env: MODEL_VERSION=v4\n",
      "env: PROJECT_ID=mwe-sanofi-ml-workshop\n"
     ]
    }
   ],
   "source": [
    "# Append date and time to model names to make them unique.\n",
    "now = !date +\"%Y%m%d_%H%M%S\"\n",
    "\n",
    "%env BUCKET_NAME=ml-workshop-chicago-taxi-demo\n",
    "%env LOCAL_JOB_DIR=local-training-output\n",
    "%env JOB_NAME=keras_job_$now.s\n",
    "%env REGION=us-central1\n",
    "# %env MODEL_NAME=keras_model_$now.s\n",
    "%env MODEL_VERSION=v4\n",
    "%env PROJECT_ID=mwe-sanofi-ml-workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://ml-workshop-chicago-taxi-demo/...\n",
      "rm: cannot remove 'input_sample.json': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Create BUCKET_NAME if it does not exist.\n",
    "!gsutil mb gs://${BUCKET_NAME}\n",
    "\n",
    "from pathlib import Path\n",
    "Path(\"./local-training-output/\").mkdir(exist_ok=True)\n",
    "\n",
    "# Remove output from previous runs, if any.\n",
    "!rm input_sample.json\n",
    "!rm x_scaler\n",
    "!rm -rf ./local-training-output/export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Perform training locally with default parameters</b>\n",
    "\n",
    "Training detail will be written locally to the folder referenced in the job-dir parameter.\n",
    "\n",
    "Note - creating the data will take some time as the MinMax normalizer needs to be fit over the 100 M plus training rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set --create-data=True once for the run of this cell.\n",
    "# !gcloud ai-platform local train \\\n",
    "#   --package-path trainer \\\n",
    "#   --module-name trainer.task \\\n",
    "#   --job-dir $LOCAL_JOB_DIR \\\n",
    "#   -- \\\n",
    "#   --project-id $PROJECT_ID \\\n",
    "#   --bucket-name ${BUCKET_NAME} \\\n",
    "#   --create-data True \\\n",
    "#   --test-files gs://${BUCKET_NAME}/data/full_test_results.csv \\\n",
    "#   --train-files gs://${BUCKET_NAME}/data/full_train_results.csv \\\n",
    "#   --eval-files gs://${BUCKET_NAME}/data/full_val_results.csv \\\n",
    "#   --num-epochs 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Perform training on AI Platform</b>\n",
    "\n",
    "The training job can also be run on AI Platform.  Note that in order for AI Platform to be able to complete the training job, the \"Google Cloud ML Engine Service Agent\" service account must be granted Cloud Storage and BigQuery admin roles.\n",
    "\n",
    "Important: A single training job (either locally or using AI Platform) must complete with the create-data flag set to true for the remainig functionality to compolete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JOB_NAME=keras_job_20200615_085701\n",
      "Job [keras_job_20200615_085701] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe keras_job_20200615_085701\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs keras_job_20200615_085701\n",
      "jobId: keras_job_20200615_085701\n",
      "state: QUEUED\n",
      "SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "now = !date +\"%Y%m%d_%H%M%S\"\n",
    "%env JOB_NAME=keras_job_$now.s\n",
    "\n",
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "  --package-path trainer/ \\\n",
    "  --module-name trainer.task \\\n",
    "  --region $REGION \\\n",
    "  --python-version 3.5 \\\n",
    "  --runtime-version 1.14 \\\n",
    "  --job-dir gs://${BUCKET_NAME}/keras-job-dir-${JOB_NAME} \\\n",
    "  -- \\\n",
    "  --project-id $PROJECT_ID \\\n",
    "  --bucket-name ${BUCKET_NAME} \\\n",
    "  --create-data True \\\n",
    "  --test-files gs://${BUCKET_NAME}/data/full_test_results.csv \\\n",
    "  --train-files gs://${BUCKET_NAME}/data/full_train_results.csv \\\n",
    "  --eval-files gs://${BUCKET_NAME}/data/full_val_results.csv \\\n",
    "  --train-steps 1 \\\n",
    "  --num-epochs 1\n",
    "                \n",
    "# Stream logs so that training is done before subsequent cells are run.\n",
    "# Remove  '> /dev/null' to see step-by-step output of the model build steps.\n",
    "!gcloud ai-platform jobs stream-logs $JOB_NAME > /dev/null\n",
    "\n",
    "# Model should exit with status \"SUCCEEDED\"\n",
    "!gcloud ai-platform jobs describe $JOB_NAME --format=\"value(state)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Perform hyperparameter tuning on AI Platform</b>\n",
    "\n",
    "Training detail will be written to Cloud Storage in the folder referenced in the job-dir parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JOB_NAME=keras_job_20200615_090309\n",
      "Job [keras_job_20200615_090309] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe keras_job_20200615_090309\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs keras_job_20200615_090309\n",
      "jobId: keras_job_20200615_090309\n",
      "state: QUEUED\n",
      "SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "now = !date +\"%Y%m%d_%H%M%S\"\n",
    "%env JOB_NAME=keras_job_$now.s\n",
    "\n",
    "!gcloud ai-platform jobs submit training ${JOB_NAME} \\\n",
    "    --config hptuning_config.yaml \\\n",
    "    --package-path trainer/ \\\n",
    "    --module-name trainer.task \\\n",
    "    --region $REGION \\\n",
    "    --python-version 3.5 \\\n",
    "    --runtime-version 1.14 \\\n",
    "    --job-dir gs://${BUCKET_NAME}/keras-job-dir-${JOB_NAME} \\\n",
    "    -- \\\n",
    "    --project-id $PROJECT_ID \\\n",
    "    --bucket-name ${BUCKET_NAME} \\\n",
    "    --create-data False \\\n",
    "    --test-files gs://${BUCKET_NAME}/data/full_test_results.csv \\\n",
    "    --train-files gs://${BUCKET_NAME}/data/full_train_results.csv \\\n",
    "    --eval-files gs://${BUCKET_NAME}/data/full_val_results.csv \\\n",
    "    --train-steps 1 \\\n",
    "    --num-epochs 1\n",
    "\n",
    "# Stream logs so that training is done before subsequent cells are run.\n",
    "# Remove  '> /dev/null' to see step-by-step output of the model build steps.\n",
    "!gcloud ai-platform jobs stream-logs ${JOB_NAME} > /dev/null\n",
    "\n",
    "# Model should exit with status \"SUCCEEDED\"\n",
    "!gcloud ai-platform jobs describe ${JOB_NAME}  --format=\"value(state)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Complete training on AI Platform</b>\n",
    "\n",
    "Now that hyperparameters have been tuned, perform deeper training with the optimal hyperparameters in place.  Note that we've explicitly increased the train-steps and num-epochs parameters in addition to the tuned hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JOB_NAME=keras_job_20200615_091137\n",
      "Job [keras_job_20200615_091137] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe keras_job_20200615_091137\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs keras_job_20200615_091137\n",
      "jobId: keras_job_20200615_091137\n",
      "state: QUEUED\n",
      "SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "# Set --create-data=False after first run. Only needs to be run once for this cell.\n",
    "now = !date +\"%Y%m%d_%H%M%S\"\n",
    "%env JOB_NAME=keras_job_$now.s\n",
    "\n",
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "  --package-path trainer/ \\\n",
    "  --module-name trainer.task \\\n",
    "  --region $REGION \\\n",
    "  --python-version 3.5 \\\n",
    "  --runtime-version 1.14 \\\n",
    "  --job-dir gs://${BUCKET_NAME}/keras-job-dir-${JOB_NAME} \\\n",
    "  -- \\\n",
    "  --project-id $PROJECT_ID \\\n",
    "  --bucket-name ${BUCKET_NAME} \\\n",
    "  --create-data False \\\n",
    "  --test-files gs://${BUCKET_NAME}/data/full_test_results.csv \\\n",
    "  --train-files gs://${BUCKET_NAME}/data/full_train_results.csv \\\n",
    "  --eval-files gs://${BUCKET_NAME}/data/full_val_results.csv \\\n",
    "  --num-deep-layers 2 \\\n",
    "  --first-deep-layer-size 5 \\\n",
    "  --first-wide-layer-size 30 \\\n",
    "  --learning-rate 0.003 \\\n",
    "  --wide-scale-factor 0.094 \\\n",
    "  --train-batch-size 132 \\\n",
    "  --dropout-rate 0.4 \\\n",
    "  --train-steps 1 \\\n",
    "  --num-epochs 1\n",
    "                \n",
    "# Stream logs so that training is done before subsequent cells are run.\n",
    "# Remove  '> /dev/null' to see step-by-step output of the model build steps.\n",
    "!gcloud ai-platform jobs stream-logs ${JOB_NAME} > /dev/null\n",
    "\n",
    "# Model should exit with status \"SUCCEEDED\"\n",
    "!gcloud ai-platform jobs describe ${JOB_NAME} --format=\"value(state)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Host the trained model on AI Platform</b>\n",
    "\n",
    "Because we're passing a list of numpy arrays and not a single numpy array as input for inference, we'll need to establish a custom prediction module.  \n",
    "\n",
    "First, execute the setup script to create a distribution tarball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sdist\n",
      "running egg_info\n",
      "writing trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to trainer.egg-info/dependency_links.txt\n",
      "writing requirements to trainer.egg-info/requires.txt\n",
      "writing top-level names to trainer.egg-info/top_level.txt\n",
      "reading manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "running check\n",
      "warning: check: missing required meta-data: url\n",
      "\n",
      "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
      "\n",
      "creating trainer-0.1\n",
      "creating trainer-0.1/trainer\n",
      "creating trainer-0.1/trainer.egg-info\n",
      "copying files to trainer-0.1...\n",
      "copying README.md -> trainer-0.1\n",
      "copying predictor.py -> trainer-0.1\n",
      "copying setup.py -> trainer-0.1\n",
      "copying trainer/__init__.py -> trainer-0.1/trainer\n",
      "copying trainer/create_data_func.py -> trainer-0.1/trainer\n",
      "copying trainer/create_scaler_func.py -> trainer-0.1/trainer\n",
      "copying trainer/model.py -> trainer-0.1/trainer\n",
      "copying trainer/task.py -> trainer-0.1/trainer\n",
      "copying trainer.egg-info/PKG-INFO -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/SOURCES.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/dependency_links.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/requires.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/top_level.txt -> trainer-0.1/trainer.egg-info\n",
      "Writing trainer-0.1/setup.cfg\n",
      "Creating tar archive\n",
      "removing 'trainer-0.1' (and everything under it)\n"
     ]
    }
   ],
   "source": [
    "!python setup.py sdist --formats=gztar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the tarball over to Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://dist/trainer-0.1.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  9.3 KiB/  9.3 KiB]                                                \n",
      "Operation completed over 1 objects/9.3 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp dist/trainer-0.1.tar.gz gs://${BUCKET_NAME}/staging-dir/trainer-0.1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a new model on AI Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MODEL_NAME=keras_model_20200615_091908\n",
      "\u001b[1;33mWARNING:\u001b[0m Using endpoint [https://ml.googleapis.com/]\n",
      "Created ml engine model [projects/mwe-sanofi-ml-workshop/models/keras_model_20200615_091908].\n"
     ]
    }
   ],
   "source": [
    "now = !date +\"%Y%m%d_%H%M%S\"\n",
    "%env MODEL_NAME=keras_model_$now.s\n",
    "!gcloud ai-platform models create $MODEL_NAME --regions $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create new version using our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Using endpoint [https://ml.googleapis.com/]\n",
      "Creating version (this might take a few minutes)......done.                    \n"
     ]
    }
   ],
   "source": [
    "!gcloud beta ai-platform versions create $MODEL_VERSION \\\n",
    "  --model $MODEL_NAME \\\n",
    "  --runtime-version 1.14 \\\n",
    "  --python-version 3.5 \\\n",
    "  --origin gs://${BUCKET_NAME}/keras-job-dir-${JOB_NAME} \\\n",
    "  --package-uris gs://${BUCKET_NAME}/staging-dir/trainer-0.1.tar.gz \\\n",
    "  --prediction-class predictor.MyPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Prepare a sample for inference</b>\n",
    "\n",
    "Note that we are using the same preprocessing methods used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Downloading: 100%|██████████████████████████████| 7/7 [00:00<00:00, 38.17rows/s]\n",
      "Downloading: 100%|██████████████████████████████| 1/1 [00:00<00:00,  6.59rows/s]\n",
      "Downloading: 100%|██████████████████████████████| 1/1 [00:00<00:00,  7.31rows/s]\n",
      "Downloading: 100%|██████████████████████████████| 1/1 [00:00<00:00,  3.24rows/s]\n",
      "Downloading: 100%|███████████████████████████| 37/37 [00:00<00:00, 130.68rows/s]\n",
      "Downloading scaler\n",
      "Downloaded scaler\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.preprocessing.data module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.preprocessing. Anything that cannot be imported from sklearn.preprocessing is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator MinMaxScaler from version 0.20.2 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "Produced sample with label 1489 seconds.\n"
     ]
    }
   ],
   "source": [
    "!python create_sample.py \\\n",
    "  --project-id $PROJECT_ID \\\n",
    "  --bucket-name ${BUCKET_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Make an inference on a new sample.</b>\n",
    "\n",
    "Pass the sample object to the model hosted in AI Platform to return a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Using endpoint [https://ml.googleapis.com/]\n",
      "{\n",
      "  \"predictions\": \"0.9210133432534364\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform predict \\\n",
    "  --model $MODEL_NAME \\\n",
    "  --version $MODEL_VERSION \\\n",
    "  --json-instances input_sample.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Approximate an Mean Absolute Percentage Error for the test set</b>\n",
    "\n",
    "Note that we used a log transformation on our target variable, so any attributes returned by the model during training will be associated with predicting the <i>log</i> of the trip duration and not the actual trip duration.  In order to calculate metrics associated with predicting the trip duration in seconds, we'll need to make predictions from the test set using our trained model.\n",
    "\n",
    "The best case scenario here would be to use the batch prediction within AI Platform.  However, batch prediction is not currently available with the custom predictor module we've implented.  \n",
    "\n",
    "As an alternative we'll approximate the MAPE by randomly sampling values from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Downloading: 100%|██████████████████████████████| 7/7 [00:00<00:00, 54.40rows/s]\n",
      "Downloading: 100%|██████████████████████████████| 1/1 [00:00<00:00,  7.87rows/s]\n",
      "Downloading: 100%|██████████████████████████████| 1/1 [00:00<00:00,  8.78rows/s]\n",
      "Downloading: 100%|██████████████████████████████| 1/1 [00:00<00:00,  8.43rows/s]\n",
      "Downloading: 100%|███████████████████████████| 37/37 [00:00<00:00, 263.14rows/s]\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.preprocessing.data module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.preprocessing. Anything that cannot be imported from sklearn.preprocessing is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator MinMaxScaler from version 0.20.2 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "\u001b[1;33mWARNING:\u001b[0m Using endpoint [https://ml.googleapis.com/]\n",
      "Returned sample with label 842 and prediction 1.\n",
      "\u001b[1;33mWARNING:\u001b[0m Using endpoint [https://ml.googleapis.com/]\n",
      "Returned sample with label 1161 and prediction 1.\n",
      "\u001b[1;33mWARNING:\u001b[0m Using endpoint [https://ml.googleapis.com/]\n",
      "Returned sample with label 763 and prediction 1.\n",
      "\u001b[1;33mWARNING:\u001b[0m Using endpoint [https://ml.googleapis.com/]\n",
      "Returned sample with label 862 and prediction 1.\n",
      "\u001b[1;33mWARNING:\u001b[0m Using endpoint [https://ml.googleapis.com/]\n",
      "Returned sample with label 663 and prediction 1.\n",
      "\u001b[1;33mWARNING:\u001b[0m Using endpoint [https://ml.googleapis.com/]\n",
      "Returned sample with label 841 and prediction 1.\n",
      "\u001b[1;33mWARNING:\u001b[0m Using endpoint [https://ml.googleapis.com/]\n",
      "Returned sample with label 531 and prediction 1.\n",
      "\u001b[1;33mWARNING:\u001b[0m Using endpoint [https://ml.googleapis.com/]\n",
      "Returned sample with label 2184 and prediction 1.\n",
      "\u001b[1;33mWARNING:\u001b[0m Using endpoint [https://ml.googleapis.com/]\n",
      "Returned sample with label 1816 and prediction 1.\n",
      "\u001b[1;33mWARNING:\u001b[0m Using endpoint [https://ml.googleapis.com/]\n",
      "Returned sample with label 809 and prediction 1.\n",
      "MAPE across 10 test samples is 100%.\n"
     ]
    }
   ],
   "source": [
    "!python calc_mape.py \\\n",
    "  --num-samples=10 \\\n",
    "  --model=$MODEL_NAME \\\n",
    "  --version=$MODEL_VERSION \\\n",
    "  --project-id $PROJECT_ID \\\n",
    "  --bucket-name ${BUCKET_NAME}"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf-gpu.1-15.m48",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf-gpu.1-15:m48"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
