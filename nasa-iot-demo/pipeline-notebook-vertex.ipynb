{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Create the template file for creating the pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Steps\n",
    "1. Define your pipeline function\n",
    "2. Build any custom components you need\n",
    "3. Use the v2 compiler to compile your code \n",
    "4. Call the gcloud API Client to establish a connection to AI Platform\n",
    "5. Run the job from the client"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Example imports\n",
    "import kfp\n",
    "import json\n",
    "import os\n",
    "import datetime\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up the environment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Defaults and environment settings\n",
    "REGION = 'us-central1'\n",
    "BUCKET_NAME = '<BUCKET_NAME>'\n",
    "ARTIFACT_STORE_URI = f'gs://{BUCKET_NAME}'\n",
    "PROJECT_ID = \"<PROJECT_ID>\"\n",
    "\n",
    "%env PROJECT_ID=$PROJECT_ID\n",
    "%env REGION=$REGION\n",
    "%env BUCKET_NAME=$BUCKET_NAME\n",
    "%env ARTIFACT_STORE_URI=$ARTIFACT_STORE_URI"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "env: ENDPOINT=https://fda9da3634d2db2-dot-us-central2.pipelines.googleusercontent.com\n",
      "env: PROJECT_ID=mwpmltr\n",
      "env: REGION=us-central1\n",
      "env: BUCKET_NAME=rrusson-bucket\n",
      "env: ARTIFACT_STORE_URI=gs://rrusson-bucket\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create the Docker images and upload to gcr.io"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "IMAGE_NAME='nasa-iot-base'\n",
    "TAG='v1'\n",
    "BASE_IMAGE='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, TAG)\n",
    "print(BASE_IMAGE)\n",
    "%env BASE_IMAGE = $BASE_IMAGE"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gcr.io/mwpmltr/nasa-iot-base:v1\n",
      "env: BASE_IMAGE=gcr.io/mwpmltr/nasa-iot-base:v1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# DON'T RUN THIS IF THE IMAGE EXISTS!\n",
    "# !gcloud builds submit --timeout 15m --tag $BASE_IMAGE base_image --async"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "IMAGE_NAME='nasa-iot-trainer'\n",
    "TAG='v5'\n",
    "TRAINER_IMAGE='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, TAG)\n",
    "print(TRAINER_IMAGE)\n",
    "%env TRAINER_IMAGE = $TRAINER_IMAGE"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gcr.io/mwpmltr/nasa-iot-trainer:v5\n",
      "env: TRAINER_IMAGE=gcr.io/mwpmltr/nasa-iot-trainer:v5\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# DON'T RUN THIS IF THE IMAGE EXISTS!\n",
    "# !gcloud builds submit --timeout 15m --tag $TRAINER_IMAGE train_image --async"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import component funcs\n",
    "NOTE: These must be imported AFTER the environment variables are set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from kfp_component.func_components import load_raw_data\n",
    "from kfp_component.func_components import split_data\n",
    "from kfp_component.func_components import vertex_custom_job"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compile the Pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Define the pipeline\n",
    "@kfp.dsl.pipeline(\n",
    "    name=\"demo-bearing-sensor-data-training\",\n",
    "    description=\"The pipeline for training and deploying an anomaly detector based on an autoencoder\",\n",
    "    pipeline_root=\"\")\n",
    "\n",
    "def pipeline(project_id: str,\n",
    "             region: str,\n",
    "             source_bucket_name: str, \n",
    "             prefix: str,\n",
    "             dest_bucket_name: str,\n",
    "             dest_file_name: str,\n",
    "             gcs_root: str,\n",
    "             dataset_location:str='US'):\n",
    "    \n",
    "    # Read in the raw sensor data from the public dataset and load in the project bucket\n",
    "    raw_data_op = load_raw_data(source_bucket_name,\n",
    "                                prefix,\n",
    "                                dest_bucket_name,\n",
    "                                dest_file_name)\n",
    "    \n",
    "       \n",
    "    # Preprocess and split the raw data by time\n",
    "    split_data_op = split_data(raw_data_op.outputs['dest_bucket_name'],\n",
    "                               raw_data_op.outputs['dest_file_name'],\n",
    "                               '2004-02-15 12:52:39',\n",
    "                               True)\n",
    "    \n",
    "    # Set up the training args\n",
    "    train_args = json.dumps(\n",
    "        [\"--bucket\", str(split_data_op.outputs['bucket_name']),\n",
    "         \"--train_file\", str(split_data_op.outputs['train_dest_file']),\n",
    "         \"--test_file\", str(split_data_op.outputs['test_dest_file']),\n",
    "         \"--job_dir\", ARTIFACT_STORE_URI,\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    job_dir = \"{0}/{1}/{2}\".format(gcs_root, 'jobdir', kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "    \n",
    "    # Train the model on AI Platform\n",
    "    train_model = vertex_custom_job(\n",
    "        project=project_id,\n",
    "        display_name=f\"anomaly-detection-{datetime.datetime.now().strftime('%H%M%S')}\",\n",
    "        container_image_uri=TRAINER_IMAGE,\n",
    "        train_args=train_args, \n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "EXPERIMENT_NAME = 'AnomalyDetector'\n",
    "RUN_ID = f\"nasa-iot-example-{datetime.datetime.now().strftime('%H%M%S')}\"\n",
    "SOURCE_BUCKET_NAME = 'amazing-public-data'\n",
    "PREFIX = 'bearing_sensor_data/bearing_sensor_data/'\n",
    "DEST_BUCKET_NAME = BUCKET_NAME\n",
    "DEST_FILE_NAME = 'raw_bearing_data.csv'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compile the pipline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "kfp.v2.compiler.Compiler().compile(pipeline, 'nasa_iot_training.json')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Submit a Run"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "pipejob = aiplatform.PipelineJob(\n",
    "    'nasa_iot_training',\n",
    "    'nasa_iot_training.json',\n",
    "    job_id=RUN_ID,\n",
    "    pipeline_root=ARTIFACT_STORE_URI,  \n",
    "    parameter_values={\n",
    "        \"project_id\": PROJECT_ID,\n",
    "        \"region\": REGION,\n",
    "        \"source_bucket_name\": SOURCE_BUCKET_NAME,\n",
    "        \"prefix\": PREFIX,\n",
    "        \"dest_bucket_name\": DEST_BUCKET_NAME,\n",
    "        \"dest_file_name\": DEST_FILE_NAME,\n",
    "        \"gcs_root\": ARTIFACT_STORE_URI,\n",
    "        \"dataset_location\": \"US\"\n",
    "    }\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "pipejob.run()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/55590906972/locations/us-central1/pipelineJobs/2750415142742130688\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/55590906972/locations/us-central1/pipelineJobs/2750415142742130688')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/2750415142742130688?project=55590906972\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/55590906972/locations/us-central1/pipelineJobs/2750415142742130688 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/55590906972/locations/us-central1/pipelineJobs/2750415142742130688 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf-gpu.1-15.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf-gpu.1-15:m59"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('nasa-iot': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "939a8dc8fcef4866de778594b036edeadbae62a1b5239c979e3add9d213c685c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}